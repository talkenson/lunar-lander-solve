# TNN//null solve of LunarLander-v2 
(Gym, OpenAI, MTS Contest - NTI)
 
 

### Промежуточные результаты
Ниже, по ссылке, представлена видео-демонстрация нашего решения задачи LunarLander-v2:
> Вот ссылочка: https://youtu.be/pzyWrRXvYT0
 
 


### Gists с результатами обучения и теста
> Обращаем Ваше внимание на то, что при дообучении агента была совершено 100 эпизодов, которые заполняли память по весам, снятым с агента, ученого на ~2000 эпизодов (2000 +- 300).
 
 


#### Сами гисты
- https://gist.github.com/tnnNull/1095b56a6291971ad70a5c0923a4555c
- https://gist.github.com/tnnNull/60b9b4f436ecc2a7ddab1901edd293f6

## Описание
### Часть первая, или как интересно мы начали понимать о чем идет речь на второй день
 
Была оформлена модель, содержащая в себе 2 скрытых слоя, 64 нейрона на каждую с функцией активации RELU
Входной слой принимал массив из observation, выходной слой на softmax, с размером action

### Часть вторая, или почему я больше не люблю обучение с подкреплением
 
Был написан массив, содержащий историю действий, представленных в виде 
> [s, a, r, s1], 
> где s - текущий state, a - совершенное действие, r - полученный reward, s1 - новое состояние, после выполнения a.

Модель была реализована на TensorFlow без использования оболочки Keras.

После чего мы тренировали агента около 2000 эпизодов.

#### Краткое описания процесса обучения
> - Загружаем наблюдения и состояние в агента, получаем на выходе некоторое действие.
> - Вводим вероятность выполнения случайного действия, вместо предложенного нейросетью. Были использованы различные константные значения вероятностей, и также линейное снижение вероятности с каждой итерацией.
> - Получаем reward за совершенное действие, помещаем в память тот самый массив ^ ([s, a, r, s1])
> - При завершении игры (возвращении средой истинного значения done = TRUE) мы обновляем веса модели, дисконтируя полученный reward, а также вводим собственные штрафы за некоторые действия (нап. зависание в воздухе в малом расстоянии от посадочной платформы)
> - Для удобства дополняем историю наград текущей, а также историю количества шагов
 - текущим
> - Каждые несколько эпизодов мы экпортируем веса модели в формате .ckpt
 
 
### Часть третья, или как производилось тестирование

Так как .ckpt в нашем случае содержит лишь частичную информацию о модели, перед началом тестирования мы заполняем память путем предтестовых эпизодов (лучший результат показан на 100 эпизодах, на меньшем количестве производится проверка)


## Полные результаты
> Вот ссылочка: https://youtu.be/oDuOOPdJ4Jw

