# TNN//null solve of LunarLander-v2 
(Gym, OpenAI, MTS Contest - NTI)
 
 

### Промежуточные результаты
Ниже, по ссылке, представлена видео-демонстрация нашего решения задачи LunarLander-v2:
> Вот ссылочка: https://youtu.be/pzyWrRXvYT0
 
 


### Gists с результатами обучения и теста
> Тестирование агента производится на весах, снятых с модели, обученной за ~2000 эпизодов (2000 +- 300).
 
 
#### Сами гисты
- https://gist.github.com/tnnNull/1095b56a6291971ad70a5c0923a4555c
- https://gist.github.com/tnnNull/60b9b4f436ecc2a7ddab1901edd293f6

## Описание
### Часть первая, или как интересно мы начали понимать о чем идет речь на второй день
 
Была оформлена модель, содержащая в себе 2 скрытых слоя, 64 нейрона на каждом с функцией активации RELU.
Входной слой принимал массив из observation, выходной слой на softmax, с размером action.

### Часть вторая, или почему я больше не люблю обучение с подкреплением
 
Был написан массив, содержащий историю действий, представленных в виде 
> [s, a, r, s1], 
> где s - текущий state, a - совершенное действие, r - полученный reward, s1 - новое состояние, после выполнения a.

Модель была реализована на TensorFlow без использования оболочки Keras.

После чего мы тренировали агента ~2000 эпизодов.

#### Краткое описания процесса обучения
> - Загружаем наблюдения и состояние в агента, получаем на выходе некоторое действие.
> - Вводим вероятность выполнения случайного действия, вместо предложенного нейросетью. Были использованы различные константные значения вероятностей, и также линейное снижение вероятности с каждой итерацией.
> - Получаем reward за совершенное действие, помещаем в память тот самый массив ^ ([s, a, r, s1])
> - При завершении игры (возвращении средой истинного значения done = TRUE) мы обновляем веса модели, дисконтируя полученный reward, а также вводим собственные штрафы за некоторые действия (нап. зависание в воздухе в малом расстоянии от посадочной платформы).
> - Для удобства дополняем историю наград текущей, а также историю количества шагов
> - Каждые несколько эпизодов мы экпортируем веса модели в формате .ckpt
 
 
### Часть третья, или как производилось тестирование
> - Создаем новую модель TensorFlow, описанную выше.
> - Загружаем веса обученной модели из формата .ckpt
> - Проводится тестирование за 100 эпизодов, при необходимости можно рендерить эпизод. На выходе получаем среднее значение за 100 эпизодов.

### Основные идеи, использованные в процессе обучения
> - Использование DeepQNeuralNetwork алгоритма, который подразумевает под собой обучение агента на Q-функции, 
> где Q-функции - функция от (s,a), возвращающая значение r после выполнения действия a в состоянии s.
> - Обучение происходит на батчах состоящих из массивов [s, a, r, s1], которые хранятся в памяти. Для этого перед запуском обучения происходит 32 случайных действия для начального заполнения памяти. В последствии случайным образом достается батч из памяти.
> - Для улучшения решения использовалась дополнительная стратегия. Агент совершал действие, предсказанное нейросетью в 95% случаях, в остальных же случаях он совершал случайное действие. Это улучшило средний результут за 100 эпизодов с 120 до 150.
> - Вторым способом улучшения было использование не константного значения вероятности совершения случайного действия, а линейного уменьшения вероятности с максимального (10%) до минимального (3%) на протяжении всего обучения. Это увеличило средниее количество очков за 100 эпизодов с 170 до 186.
> - Третьей идеей для улучшения было использование не линейного уменьшения вероятности, а экспоненциального. Но данное улучшение особо не повлияло на финальный результат обучения, за 100 эпизодов тестирования средний результат со 186 увеличился до 191, т.е. незначительно.
> - Четвертая идея - введение дополнительного штрафования за зависания в воздухе. Одной из проблем обучения было то, что Агент зависал над платформой и не садился на нее. Введение штрафа за создание таких ситуаций позволило значительно увеличить средний результат на тестах с 191 до 250.



## Полные результаты
> Вот ссылочка: https://youtu.be/oDuOOPdJ4Jw

